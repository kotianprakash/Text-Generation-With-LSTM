{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "from pickle import dump,load\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM,Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    '''\n",
    "    Function to read file from a given path\n",
    "    '''\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punc(doc_text):\n",
    "    '''\n",
    "    Function to take only document words that are not punctuation\n",
    "    '''\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    '''\n",
    "    Function to create a LSTM model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate (50 words in the video)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict(pad_encoded)\n",
    "#         print(pred_word_ind)\n",
    "        pred_word_ind = np.argmax(pred_word_ind,axis=1)[0]\n",
    "#         pred_word_ind = np.round(pred_word_ind).astype(int)[0]\n",
    "#         pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        print(pred_word_ind)\n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.disable_pipes('ner', 'tagger', 'parser')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_lenght = 1198623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_file(r\"C:\\Users\\prakash kotian\\Desktop\\Data_Science\\Deep Learning\\NLP\\NLP Basic\\Text Generation with LSTM\\moby_dick_four_chapters.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The text from our Dataset : \n",
      "\n",
      "Call me Ishmael.  Some years ago--never mind how long\n",
      "precisely--having little or no money in my purse, and nothing\n",
      "particular to interest me on shore, I thought I would sail about a\n",
      "little and see the watery part of the world.  It is a way I have of\n",
      "driving off the spleen and regulating the circulation.  Whenever I\n",
      "find myself growing grim about the mouth; whenever it is a damp,\n",
      "drizzly November in my soul; whenever I find myself involuntarily\n",
      "pausing before coffin warehouses, and bringing up t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe text from our Dataset : \\n\")\n",
    "print(dataset[0:500])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = seperate_punc(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a batch of sequence with 26 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing the chapters into sequences of 26 words - \n",
      "\n",
      "\n",
      "Text Sequence 1 - \n",
      " call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on \n",
      "\n",
      "Text Sequence 2 - \n",
      " me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_length = 25+1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(training_length, len(tokens)):\n",
    "    seq = tokens[i-training_length:i]\n",
    "    text_sequences.append(seq)\n",
    "print(\"Dividing the chapters into sequences of 26 words - \\n\\n\")\n",
    "print(f\"Text Sequence 1 - \\n {' '.join(text_sequences[0])} \\n\")\n",
    "print(f\"Text Sequence 2 - \\n {' '.join(text_sequences[1])} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,  263,   51,  261,  408,   87,  219,  129,  111,  954,  260,\n",
       "         50,   43,   38,  314,    7,   23,  546,    3,  150,  259,    6,\n",
       "       2713,   14,   24,  957])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divinding into X and y data\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 50)            15200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              138669    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,594\n",
      "Trainable params: 244,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating a model by calling create_model method\n",
    "model = create_model(vocabulary_size+1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "89/89 [==============================] - 3s 35ms/step - loss: 5.9662 - accuracy: 0.0598\n",
      "Epoch 2/8\n",
      "89/89 [==============================] - 3s 38ms/step - loss: 5.8931 - accuracy: 0.0636\n",
      "Epoch 3/8\n",
      "89/89 [==============================] - 3s 37ms/step - loss: 5.8091 - accuracy: 0.0653\n",
      "Epoch 4/8\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 5.7371 - accuracy: 0.0665\n",
      "Epoch 5/8\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 5.6577 - accuracy: 0.0669\n",
      "Epoch 6/8\n",
      "89/89 [==============================] - 4s 41ms/step - loss: 5.5901 - accuracy: 0.0678\n",
      "Epoch 7/8\n",
      "89/89 [==============================] - 4s 39ms/step - loss: 5.5328 - accuracy: 0.0701\n",
      "Epoch 8/8\n",
      "89/89 [==============================] - 3s 36ms/step - loss: 5.4803 - accuracy: 0.0737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2473aa16748>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model.fit(X, y, batch_size=128, epochs=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and tokenizer\n",
    "model.save('my_mobydick_model.h5')\n",
    "dump(tokenizer, open('my_simpletokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "65\n",
      "\n",
      "\n",
      " The input text : \n",
      " thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\n",
      "\n",
      "\n",
      " The predicted text (next 15 words) is : \n",
      " the little night of the little night of the little night of the little night\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "50\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "65\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "50\n",
      "\n",
      "\n",
      " The input text : \n",
      " had sounded my pocket and only brought up a few pieces of silver,--so wherever you go ishmael said i to myself as i stood in the\n",
      "\n",
      "\n",
      " The predicted text (next 25 words) is : \n",
      " little night of the little night of the little night of the little night of the little night of the little night of the little\n"
     ]
    }
   ],
   "source": [
    "# creating a random sequence\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0, len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "seed_text = ' '.join(random_seed_text)\n",
    "pred_words = generate_text(model, tokenizer, seq_len, seed_text, num_gen_words=15)\n",
    "print(f\"\\n\\n The input text : \\n {seed_text}\")\n",
    "print(f\"\\n\\n The predicted text (next 15 words) is : \\n {pred_words}\")\n",
    "seed_text = ' '.join(text_sequences[2565])\n",
    "pred_words = generate_text(model, tokenizer, seq_len, seed_text, num_gen_words=25)\n",
    "print(f\"\\n\\n The input text : \\n {seed_text}\")\n",
    "print(f\"\\n\\n The predicted text (next 25 words) is : \\n {pred_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
